# Default values


# Configure external database host
# If we provide an external dbHost , it will not create standalone db
# dbHost: ""
# dbPort: 3306
# dbRootUser: "admin"

# If we provide dbRootPassword ,it will create a secret for dbRootPassword
# dbRootPassword: ""
# dbRds: true

nameOverride: "erpnext" # this will be used as label eg.app.kubernetes.io/name: {{ include "erpnext.name" . }}-worker-l
fullnameOverride: "erpnext"  # this will be used as a prefix for the resource name 


secretproviderclass:
  enabled: false
  SecretProviderName: "erpnext-spc"
  SecretProvider: "aws"
  secretName: "erpnext-secrets"
  secrets:
    - objectName: "erpnext/secrets"
      keys:
        - path: "dbRootPassword"
          value: "db-root-password"

image:
  repository:  harshjoundal7/frappelms
  tag: v3
  pullPolicy: IfNotPresent

# Any new site added in sitesConfig will be created while new-site jobs runs automatically
# on same instance.
# Frappe supports multi-tenant architecture, so we can have multiple sites in same instance
sitesConfig:
  - siteName: erpnext.harshjoundal.in
    enabled: true
    dbName: erpnext # db name for site should be unique as it will be created in same cluster
    forceCreate: false # do not force create if site aleady exist , it will override and create new site
    #These apps will be installed while site creation 
    #also if we add new apps in future,it will install those apps
    # while site migration 
    installApps:
      - erpnext
      - special_ops_frappe_utilities
      - frappe_external_solutions
    uninstallApps: [] #These apps will be uninstalled when uninstallApps job runs
    backup: # If enabled site will be backed up while backup job runs
      enabled: false

commonEnvVars: &commonEnvVars
  - name: AWS_REGION
    value: "ap-south-1"
  - name: AWS_BUCKET
    value: "erpnext-dev"

nginx:
  replicaCount: 2
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    targetCPU: 75
    targetMemory: 75
  # config: |
  #   # custom conf /etc/nginx/conf.d/default.conf
  environment:
    upstreamRealIPAddress: "127.0.0.1"
    upstreamRealIPRecursive: "off"
    upstreamRealIPHeader: "X-Forwarded-For"
    frappeSiteNameHeader: "$host"
  livenessProbe:
    tcpSocket:
      port: 8080
    initialDelaySeconds: 5
    periodSeconds: 10
  readinessProbe:
    tcpSocket:
      port: 8080
    initialDelaySeconds: 5
    periodSeconds: 10
  service:
    type: ClusterIP
    port: 8080
  resources: 
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  annotations:
    scheduling.cast.ai/spot-percentage: "70"
  nodeSelector: {}
  tolerations: []
  affinity: {}
  envVars: []
  initContainers: []
  sidecars: []

worker:
  gunicorn:
    replicaCount: 2
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 5
      targetCPU: 75
      targetMemory: 75
    livenessProbe:
      tcpSocket:
        port: 8000
      initialDelaySeconds: 5
      periodSeconds: 10
    readinessProbe:
      tcpSocket:
        port: 8000
      initialDelaySeconds: 5
      periodSeconds: 10
    service:
      type: ClusterIP
      port: 8000
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 1Gi
    nodeSelector: {}
    annotations:
      scheduling.cast.ai/spot-percentage: "70"
      harsh: "hehe"
    tolerations: []
    affinity: {}
    args: []
    envVars: []
    initContainers: []
    sidecars: []

  default:
    replicaCount: 1
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 3
      targetCPU: 75
      targetMemory: 75
    resources: {}
    annotations:
      scheduling.cast.ai/spot-percentage: "70"
    nodeSelector: {}
    tolerations: []
    affinity: {}
    livenessProbe:
      override: false
      probe: {}
    readinessProbe:
      override: false
      probe: {}
    envVars: []
    initContainers: []
    sidecars: []

  short:
    replicaCount: 1
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 3
      targetCPU: 75
      targetMemory: 75
    resources: {}
    annotations:
      scheduling.cast.ai/spot-percentage: "70"
    nodeSelector: {}
    tolerations: []
    affinity: {}
    livenessProbe:
      override: false
      probe: {}
    readinessProbe:
      override: false
      probe: {}
    envVars: []
    initContainers: []
    sidecars: []

  long:
    replicaCount: 1
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 3
      targetCPU: 75
      targetMemory: 75
    resources: {}
    annotations:
      scheduling.cast.ai/spot-percentage: "70"
    nodeSelector: {}
    tolerations: []
    affinity: {}
    livenessProbe:
      override: false
      probe: {}
    readinessProbe:
      override: false
      probe: {}
    envVars: []
    initContainers: []
    sidecars: []

  scheduler:
    replicaCount: 1
    resources: {}
    annotations:
      scheduling.cast.ai/spot-percentage: "70"
    nodeSelector: {}
    tolerations: []
    affinity: {}
    livenessProbe:
      override: false
      probe: {}
    readinessProbe:
      override: false
      probe: {}
    envVars: []
    initContainers: []
    sidecars: []

  healthProbe: |
    exec:
      command:
        - bash
        - -c
        - echo "Ping backing services";
        {{- if .Values.mariadb.enabled }}
        {{- if eq .Values.mariadb.architecture "replication" }}
        - wait-for-it {{ .Release.Name }}-mariadb-primary:{{ .Values.mariadb.primary.service.ports.mysql }} -t 1;
        {{- else }}
        - wait-for-it {{ .Release.Name }}-mariadb:{{ .Values.mariadb.primary.service.ports.mysql }} -t 1;
        {{- end }}
        {{- else if .Values.dbHost }}
        - wait-for-it {{ .Values.dbHost }}:{{ .Values.mariadb.primary.service.ports.mysql }} -t 1;
        {{- end }}
        {{- if index .Values "redis-cache" "enabled" }}
        - wait-for-it {{ .Release.Name }}-redis-cache-master:{{ index .Values "redis-cache" "master" "containerPorts" "redis" }} -t 1;
        {{- else if index .Values "redis-cache" "host" }}
        - wait-for-it {{ index .Values "redis-cache" "host" }} -t 1;
        {{- end }}
        {{- if index .Values "redis-queue" "enabled" }}
        - wait-for-it {{ .Release.Name }}-redis-queue-master:{{ index .Values "redis-queue" "master" "containerPorts" "redis" }} -t 1;
        {{- else if index .Values "redis-queue" "host" }}
        - wait-for-it {{ index .Values "redis-queue" "host" }} -t 1;
        {{- end }}
    initialDelaySeconds: 15
    periodSeconds: 5

socketio:
  replicaCount: 2
  autoscaling:
    enabled: false
    minReplicas: 2
    maxReplicas: 3
    targetCPU: 75
    targetMemory: 75
  livenessProbe:
    tcpSocket:
      port: 9000
    initialDelaySeconds: 5
    periodSeconds: 10
  readinessProbe:
    tcpSocket:
      port: 9000
    initialDelaySeconds: 5
    periodSeconds: 10
  resources: {}
  annotations:
    scheduling.cast.ai/spot-percentage: "70"
  nodeSelector: {}
  tolerations: []
  affinity: {}
  service:
    type: ClusterIP
    port: 9000
  envVars: []
  initContainers: []
  sidecars: []

# PVC configuration
persistence:
  worker:
    enabled: true
    accessModes:
      - ReadWriteOnce
    # existingClaim: ""
    size: 8Gi
    storageClass: "standard"
    # storageClass: "efs-sc-v2"
    # storageClass: "longhorn"

# Ingress
ingress:
  # ingressName: ""
  # className: ""
  enabled: false
  annotations:
    kubernetes.io/ingress.class: traefik
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    # cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
  - host: erpnext.harshjoundal.in
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
  #  - secretName: auth-server-tls
  #    hosts:
  #      - auth-server.local

jobs:
  volumePermissions:
    enabled: false
    backoffLimit: 0
    resources: {}
    annotations: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}

  configure:
    enabled: true
    fixVolume: true
    backoffLimit: 0
    resources: {}
    annotations: 
      "argocd.argoproj.io/hook": "Sync"
      "argocd.argoproj.io/sync-wave": "0"
      
      # ArogoCd doesnt works well with helm hooks, use below for local testing
      # "helm.sh/hook": "post-install,post-upgrade"
      # "helm.sh/hook-weight": "1"
      # "helm.sh/hook-delete-policy": "before-hook-creation"
    nodeSelector: {}
    tolerations: []
    affinity: {}
    envVars: []
    command: []
    args: []
    extraArgs: 
      - bench set-config -g allow_cors "*"

  createSite:
    enabled: true
    forceCreate: true
    siteName: "erp.cluster.local"
    adminPassword: "changeit"
    dbType: "mariadb"
    backoffLimit: 0
    resources: {}
    annotations: 
      "argocd.argoproj.io/hook": "Sync"
      "argocd.argoproj.io/sync-wave": "1"
      # "helm.sh/hook": "post-install,post-upgrade"
      # "helm.sh/hook-weight": "1"
      # "helm.sh/hook-delete-policy": "before-hook-creation"
    nodeSelector: {}
    tolerations: []
    affinity: {}

  dropSite:
    enabled: false
    forced: false
    siteName: "erpnext.harshjoundal.in"
    backoffLimit: 0
    resources: {}
    annotations: 
      "argocd.argoproj.io/hook": "Sync"
      "argocd.argoproj.io/sync-wave": "3"
      # "helm.sh/hook": "post-install,post-upgrade"
      # "helm.sh/hook-weight": "3"
      # "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
    nodeSelector: {}
    tolerations: []
    affinity: {}

  backup:
    enabled: false
    withFiles: true
    backoffLimit: 0
    resources: {}
    annotations: 
      "argocd.argoproj.io/hook": "Sync"
      "argocd.argoproj.io/sync-wave": "2"
      # "helm.sh/hook": "post-install,post-upgrade"
      # "helm.sh/hook-weight": "3"
      # "helm.sh/hook-delete-policy": "before-hook-creation"
    nodeSelector: {}
    tolerations: []
    affinity: {}

  migrate:
    enabled: true
    skipFailing: false
    postMigrateCommands : []
    backoffLimit: 0
    annotations:
      "argocd.argoproj.io/hook": "Sync"
      "argocd.argoproj.io/sync-wave": "5"
      # "helm.sh/hook": "post-install,post-upgrade"
      # "helm.sh/hook-weight": "6"
      # "helm.sh/hook-delete-policy": "before-hook-creation"
    resources: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}

  custom:
    enabled: false
    jobName: ""
    labels: {}
    backoffLimit: 0
    initContainers: []
    containers: []
    restartPolicy: Never
    annotations: {}
    volumes: []
    nodeSelector: {}
    affinity: {}
    tolerations: []

  uninstallApps:
    enabled: true
    jobName: ""
    backoffLimit: 0
    forced: false
    resources: {}
    annotations: 
      "argocd.argoproj.io/hook": "Sync"
      "argocd.argoproj.io/sync-wave": "4"
      "argocd.argoproj.io/hook-delete-policy": "HookSucceeded,BeforeHookCreation"
      # "helm.sh/hook": "post-install,post-upgrade"
      # "helm.sh/hook-weight": "4"
      # "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
    nodeSelector: {}
    tolerations: []
    affinity: {}

imagePullSecrets: []

# ----------------------  Service Account  ----------------------
# This servie
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  annotations: {}

podSecurityContext:
  supplementalGroups: [1000]

securityContext:
  capabilities:
    add:
    - CAP_CHOWN
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# ----------------------  REDIS  ----------------------
# If we are using external redis , disable this and provide the host so it will not create standalone redis
redis-cache:
  # host: "redis://13.201.56.51:6379"
  enabled: true #if we have external redis, disable this and provide the extenal host
  architecture: standalone
  auth:
    enabled: false
    sentinal: false
  master:
    containerPorts:
      redis: 6379
    persistence:
      enabled: false

redis-queue:
  # host: "redis://13.201.56.51:6379"
  enabled: true #if we have external redis, disable this and provide the extenal host
  architecture: standalone
  auth:
    enabled: false
    sentinal: false
  master:
    containerPorts:
      redis: 6379
    persistence:
      enabled: false

# --------------------- DB ----------------------
# If we are using external db , disable this so it will not create standalone db
mariadb:
  # https://github.com/bitnami/charts/tree/master/bitnami/mariadb
  enabled: true
  auth:
    rootPassword: "changeit"
    username: "erpnext"
    password: "changeit"
    replicationPassword: "changeit"
  primary:
    service:
      ports:
        mysql: 3306
    extraFlags: >-
      --skip-character-set-client-handshake
      --skip-innodb-read-only-compressed
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_unicode_ci

